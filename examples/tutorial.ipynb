{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic workflow for using CGT is as follows.\n",
    "\n",
    "1.) Define symbolic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cgt\n",
    "a = cgt.scalar(name='a') # float-valued scalar, with optional name provided\n",
    "b = cgt.scalar(name='b')\n",
    "n = cgt.scalar(name='n', dtype='int64') # integer scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) Construct a expression out of these variables. Operators like `+,<,**` are overloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = (a**n + b**n)**(1.0/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.) Construct a ``function``, which maps numerical inputs to numerical outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = cgt.function([a,b,n], c)\n",
    "print f(8,15,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 2, when you constructed the expression ``c``, CGT was building up a data structure that stores the set of operations needed to compute ``c`` in terms of the input arguments. To be precise, CGT builds up a directed acyclic graph describing the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cgt.as_dot(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the labels 0 and 1 indicate whether the edge corresponds to the zeroth or first argument to the function.\n",
    "\n",
    "With this simple example out of the way, let's move on to vectors, matrices and gradients.\n",
    "We'll first consider linear regression, where we have a linear model, $y_{pred} = Xw + b$, and a quadratic loss: $L(X,y,w,b) = \\|y - y_{pred}\\|^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_nk = cgt.matrix(\"X\")\n",
    "y_n = cgt.vector(\"y\")\n",
    "w_k = cgt.vector(\"w\")\n",
    "b = cgt.scalar(\"b\")\n",
    "ypred_n = X_nk.dot(w_k) + b\n",
    "L = cgt.sum(cgt.square(ypred_n - y_n))\n",
    "print \"L = \",\n",
    "cgt.print_expr(L);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first four lines, we created symbolic variables ``X_nk, y_n, w_k, b``.\n",
    "Then, we constructed more complex expressions by applying functions to these symbolic variables, finally giving us the loss function ``L``.\n",
    "\n",
    "You can find the available functions by inspecting at the `cgt` namespace (type `cgt.<tab>`), and you'll find lots of familiar NumPy functions. Note that these symbolic variables have some of the same attributes as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print X_nk.ndim, str(X_nk.shape), X_nk.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can compute the gradient of the loss function with respect to the parameters $W,b$. (Note, however, that there's nothing special about parameters: we could just as well compute the gradient with respect to $X$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = dLdw, dLdb = cgt.grad(L, [w_k, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, ``dLdw`` and ``dLdb`` are just expressions. When we called ``cgt.grad``, CGT walked through the graph that represents the expression ``L`` and constructed expressions for the gradients. This procedure is an variant of automatic differentiation or the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Loss and gradient objects\", dLdw, dLdb\n",
    "print \"Pretty-printed gradient: \", \n",
    "cgt.print_expr(cgt.simplify([dLdw])[0]);\n",
    "cgt.as_dot(cgt.simplify([dLdw]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compile functions to compute the loss and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_loss = cgt.function([X_nk, y_n, w_k, b], L)\n",
    "f_grads = cgt.function([X_nk, y_n, w_k, b], grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some random numerical data and parameter values so we can test out these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as nr\n",
    "nr.seed(0)\n",
    "# Generate some random data\n",
    "Xval = nr.randn(100,3)\n",
    "yval = nr.randn(100)\n",
    "# Initialize parameters\n",
    "wval = nr.randn(3)\n",
    "bval = nr.randn()\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print \"loss:\", f_loss(Xval, yval, wval, bval)\n",
    "print \"grad:\", f_grads(Xval, yval, wval, bval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can compute the gradient, we're ready to do some optimization, where we minimize the loss with respect to $W,b$. There are many different styles for doing numerical optimization with CGT. \n",
    "\n",
    "We'll start with a style that is convenient but not the most efficient for large-scale machine learning. Namely, we'll flatten and concatenate the parameters into one vector $\\theta$, and we'll return a flat gradient vector.\n",
    "Then we'll hand off our functions to scipy's BFGS optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(theta): \n",
    "    return f_loss(Xval, yval, theta[0:3], theta[3])\n",
    "def gradf(theta):\n",
    "    gw,gb = f_grads(Xval, yval, theta[0:3], theta[3])\n",
    "    return np.concatenate([gw,gb.reshape(1)])\n",
    "import scipy.optimize as opt\n",
    "theta_opt = opt.fmin_bfgs(f, np.zeros(4), gradf, disp=False)\n",
    "print \"Optimal theta:\", theta_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's just make sure we got the right answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Least squares solution:\",np.linalg.lstsq(np.concatenate([Xval, np.ones((len(Xval),1))],axis=1), yval)[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do the flattening and unflattening using CGT, which will usually be faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = cgt.vector('theta')\n",
    "w_k_1 = theta[0:3]\n",
    "b_1 = theta[3]\n",
    "ypred_n_1 = X_nk.dot(w_k_1) + b_1\n",
    "L_1 = cgt.sum(cgt.square(ypred_n_1 - y_n))\n",
    "dLdtheta, = cgt.grad(L_1, [theta])\n",
    "\n",
    "f = cgt.function([theta], L_1, givens = [(X_nk,Xval), (y_n,yval)])\n",
    "gradf = cgt.function([theta], dLdtheta, givens = [(X_nk,Xval), (y_n,yval)])\n",
    "\n",
    "theta_opt = opt.fmin_bfgs(f, np.zeros(4), gradf, disp=False)\n",
    "print \"Optimal theta:\", theta_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we provided one additional argument to ``cgt.function`` above: ``givens``. The value provided should be a list of pairs (input variable, replacement), where replacement can be a numerical value or some other symbolic variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll introduce another way of doing optimization, but here we will be able to do in-place updates of our parameters, which will be useful for large-scale problems. This style is especially useful when using the GPU, to avoid transfering data to and from the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stepsize=0.001\n",
    "w_k_2 = cgt.shared(wval.copy(), name='w')\n",
    "b_2 = cgt.shared(bval, name='b')\n",
    "ypred_n_2 = X_nk.dot(w_k_2) + b_2\n",
    "L_2 = cgt.sum(cgt.square(ypred_n_2 - y_n))\n",
    "dLdw_2,dLdb_2 = cgt.grad(L_2, [w_k_2, b_2])\n",
    "\n",
    "updates = [(w_k_2, w_k_2 - stepsize*dLdw_2), (b_2, b_2 - stepsize*dLdb_2)]\n",
    "givens =  [(X_nk,Xval), (y_n,yval)]\n",
    "f_update = cgt.function([], L_2, givens = givens, updates = updates)\n",
    "for i in xrange(100):\n",
    "    f_update()\n",
    "print w_k_2.get_value(), b_2.get_value()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
